---
output:
  html_document: default
  Author: default
  pdf_document: default
  word_document: default
---
# Step 1

#### Reading Data into R
```{r eval=TRUE}
datapath<-"C:/MScA/Statistical Analysis/course project"
AssignmentData<- read.csv(file=paste(datapath,"regressionassignmentdata2014.csv",sep="/"),row.names=1,header=TRUE,sep=",")
head(AssignmentData)
```

#### Plotting Inputs
```{r eval=TRUE,echo=FALSE,fig.width=7,fig.height=5}
matplot(AssignmentData[,-c(8,9,10)],type='l')
```

#### Plotting Input along with unknown Output
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,-c(9,10)],type='l')
```

#Step 2

### 2.1 Estimating simple regression model with each of the input variables against the unknown output variable

```{r eval=TRUE}
model3M<-lm(Output1~USGG3M,data=AssignmentData)
model6M<-lm(Output1~USGG6M,data=AssignmentData)
model2YR<-lm(Output1~USGG2YR,data=AssignmentData)
model3YR<-lm(Output1~USGG3YR,data=AssignmentData)
model5YR<-lm(Output1~USGG5YR,data=AssignmentData)
model10YR<-lm(Output1~USGG10YR,data=AssignmentData)
model30YR<-lm(Output1~USGG30YR,data=AssignmentData)
```


### 2.2 Analyzing the fit Models

####For model3M  
```{r eval=TRUE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model3M)$sigma^2, Coeff=coefficients(model3M),R2=summary(model3M)$r.squared)
```


Plotting against unknown Output and Analyzing Residuals  
```{r eval=TRUE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model3M$fitted.values,col="red")
resids<-model3M$residuals
plot(AssignmentData$USGG3M,resids)
```
  
1. From table above it appears that this predictor is a good predictor of the output with a R2 value of 0.9628  
2.The explained variance is thus much larger than the unexplained variance  
3.The coefficient suggests that a unit % increase in the 3 Month yield rate will increase the output by 2.5%  
4.The plot of residuals suggests a non-normal distribution with variations along the timeline. This could be because of extrinsic market factirs affecting the output differently at different points of time.The mean of residuals is close to zero.  

####For model6M

  (Skipped Code)  
```{r eval=TRUE, echo=FALSE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model6M)$sigma^2, Coeff=coefficients(model6M),R2=summary(model6M)$r.squared)
```

Plotting against unknown Output and Analyzing Residuals

  (Skipped Code)  
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model6M$fitted.values,col="red")
resids<-model6M$residuals
plot(AssignmentData$USGG6M,resids)
```

  The conclusions that can be drawn from this model are very similar to the ones drawn from the model with 3 months' yield as input  
1.The predictor is a good predictor of the output with a R2 value of 0.9744  
2.The explained variance is thus much larger than the unexplained variance  
3.The coefficient suggests that a unit % increase in the 3 Month yield rate will increase the output by 2.49%  
4.The plot of residuals suggests a non-normal distribution with variations along the timeline due to same reasons as above.  

In both the cases the intercept is negative suggesting that at 0% yield on these two inputs, the output is negative.  


####For model2YR

  (skipped Code)    
```{r eval=TRUE, echo=FALSE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model2YR)$sigma^2, Coeff=coefficients(model2YR),R2=summary(model2YR)$r.squared)
```
Plotting against unknown Output and Analyzing Residuals

  (Skipped Code)    
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model2YR$fitted.values,col="red")
resids<-model2YR$residuals
plot(AssignmentData$USGG2YR,resids)
```
    
  1. The plot of the fitted values against output seems to be an almost exact fit suggesting that the unkown output may be even predicted by this predictor alone.  
2.The R2 value of 0.9966 also supports this conviction. This is further strengthened by looking at the very small portion of the variance that remains unexplained.  
3.The coefficient of 2.40 suggests a positive increase of 2.4% per unit increase in yield of 2YR bonds.  
4.The residuals continue to behave the same way as in other models  

####For model3YR

  (skipped Code)  
```{r eval=TRUE, echo=FALSE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model3YR)$sigma^2, Coeff=coefficients(model3YR),R2=summary(model3YR)$r.squared)
```
Plotting against unknown Output and Analyzing Residuals

  (Skipped Code)  
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model3YR$fitted.values,col="red")
resids<-model3YR$residuals
plot(AssignmentData$USGG3YR,resids)
qqnorm(resids);qqline(resids)
```
  
1. This appears to be very closely in line with the 2YR bonds' yield as input and actually outperforms in terms of portion of variance explained and therefore has a higher R2 of 0.9979.  
2.The intercept is negative and comparable to previous case while the slope is slightly higher suggesting greater increase in output per unit increase in the yield 
3.The residuals here behave almost close to being uniformly/normally distributed for the most part. It appears that the sub-prime crisis threw it off causing some anomaly in that period  

####For model5YR

(skipped Code)
```{r eval=TRUE, echo=FALSE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model5YR)$sigma^2, Coeff=coefficients(model5YR),R2=summary(model5YR)$r.squared)
```
  
Plotting against unknown Output and Analyzing Residuals  

(Skipped Code)  
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model5YR$fitted.values,col="red")
resids<-model5YR$residuals
plot(AssignmentData$USGG5YR,resids)
```
  
  1. The portion of variance explained falls marginally to 0.9917 (R2)  
2. The slope increased to 2.57 suggesting this predictor has a greater impact on per unit output as compared to previous  
3.The intercept is the most negative so far suggesting in its absence the output is negatively impacted the furthest  
4.The residuals again have a tendency to change across time intervals with deviance from the normal distribution  


####For model10YR  

(skipped Code)  
```{r eval=TRUE, echo=FALSE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model10YR)$sigma^2, Coeff=coefficients(model10YR),R2=summary(model10YR)$r.squared)
```

Plotting against unknown Output and Analyzing Residuals  

(Skipped Code)  
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model10YR$fitted.values,col="red")
resids<-model10YR$residuals
plot(AssignmentData$USGG10YR,resids)
```
  
  1. The 10YR yield explains 96.91% of the variance. This intself is a strong predictor, however compared to previous predictors, the portion of unexplained variance has increased  
2.The coefficienst suggest that a unit increase in the 10YR bond yield has a larger (2.79x) increase in output  
3.The residuals continue to be away from the normal distribution and varying over time with approximately 0 mean value  

####For model30YR  

(skipped Code)  
```{r eval=TRUE, echo=FALSE}
c(Total.Variance=var(AssignmentData[,8]),Unexplained.Variance=summary(model30YR)$sigma^2, Coeff=coefficients(model30YR),R2=summary(model30YR)$r.squared)
```
Plotting against unknown Output and Analyzing Residuals  

(Skipped Code)  
```{r eval=TRUE,echo=FALSE}
matplot(AssignmentData[,8],type="l",xaxt="n")
lines(model30YR$fitted.values,col="red")
resids<-model30YR$residuals
plot(AssignmentData$USGG30YR,resids)
```
    
  1. This is individually a further weaker predictor than all the previous predictors considered with 93.53% of the variance explained  
2.The intercept here is the most negative and the slope the most positive- suggesting that its absence leads to the most negative output while its presence results in the greatest per unit increase in output  
3.The residuals are again quite random, non-normal with high deviation from the normal and variance across time.  

####For all of the models above, the intercept value is negative. However, if output is a value such that it cannot be negative (like revenue for example), then it would mean that the curve deviates from a straight line in the initial years  


###2.3 Combining into single Apply function

```{r eval=TRUE}
IPasIP<-apply(AssignmentData[,1:7],2,function (SlopeOfMaturity) lm (AssignmentData$Output1~SlopeOfMaturity)$coefficients)
IPasIP
```


#Step 3

### 3.1 Estimating simple regression model withthe output as input against each of the input variables as output

```{r eval=TRUE}
model3MFlip<-lm(USGG3M~Output1,data=AssignmentData)
model6MFlip<-lm(USGG6M~Output1,data=AssignmentData)
model2YRFlip<-lm(USGG2YR~Output1,data=AssignmentData)
model3YRFlip<-lm(USGG3YR~Output1,data=AssignmentData)
model5YRFlip<-lm(USGG5YR~Output1,data=AssignmentData)
model10YRFlip<-lm(USGG10YR~Output1,data=AssignmentData)
model30YRFlip<-lm(USGG30YR~Output1,data=AssignmentData)
```

###3.2 Combining into single Apply function

```{r eval=TRUE}
SlopeOfUnknownOuput1<-AssignmentData$Output1
OPasIP<-apply(AssignmentData[,1:7],2,function (x) lm (x~SlopeOfUnknownOuput1)$coefficients)
OPasIP
```

#Step 4

###Estimate logistic regression using all inputs and the data on FED tightening and easing cycles.
```{r eval=T}
AssignmentDataLogistic<-data.matrix(AssignmentData,rownames.force="automatic")
head(AssignmentDataLogistic)
```
#### Create columns of easing periods (as 0s) and tightening periods (as 1s)
```{r eval=T}
EasingPeriods<-AssignmentDataLogistic[,9]
EasingPeriods[AssignmentDataLogistic[,9]==1]<-0
TighteningPeriods<-AssignmentDataLogistic[,10]
```

#### Check easing and tightening periods
```{r eval=T}
cbind(EasingPeriods,TighteningPeriods)[c(550:560,900:910,970:980),]
```

#### Remove the periods of neither easing nor tightening.

```{r eval=TRUE,results='hide'}
All.NAs<-is.na(EasingPeriods)&is.na(TighteningPeriods)
AssignmentDataLogistic.EasingTighteningOnly<-AssignmentDataLogistic
AssignmentDataLogistic.EasingTighteningOnly[,9]<-EasingPeriods
AssignmentDataLogistic.EasingTighteningOnly<-AssignmentDataLogistic.EasingTighteningOnly[!All.NAs,]
AssignmentDataLogistic.EasingTighteningOnly[is.na(AssignmentDataLogistic.EasingTighteningOnly[,10]),10]<-0
AssignmentDataLogistic.EasingTighteningOnly[,10]==1
#Binary output for logistic regression is now in column 10
```

```{r eval=TRUE,echo=FALSE}
matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)],type="l",ylab="Data and Binary Fed Mode")
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20,col="red")
```
  
### 4.1 and 4.2 Logistic Regression, AIC,Coefficients and Graph

```{r eval=T}
LogisticModel.TighteningEasing_All<-glm(AssignmentDataLogistic.EasingTighteningOnly[,10]~                                      AssignmentDataLogistic.EasingTighteningOnly[,1:7],family=binomial(link=logit))
#summary(LogisticModel.TighteningEasing_All)
print(paste0("AIC VAlues is ",summary(LogisticModel.TighteningEasing_All)$aic))
summary(LogisticModel.TighteningEasing_All)$coefficients[,c(1,4)]

```

###Plot dividing line

```{r eval=T}
matplot(AssignmentDataLogistic.EasingTighteningOnly[,-c(9,10)],type="l",ylab="Results of Logistic Regression")
lines(AssignmentDataLogistic.EasingTighteningOnly[,10]*20,col="red")
lines(LogisticModel.TighteningEasing_All$fitted.values*20,col="green")
```

### Calculate odds
```{r eval=T}
Log.Odds<-predict(LogisticModel.TighteningEasing_All)
plot(Log.Odds,type="l")
```
  
### 4.3 Interpretation of Logistic Regression
  

The estimate of each of the predictors is a measure of the amount my which the natural log of odds changes per unit change in the value of the predictor.  
Consider the example of the 3Month yield, the estimate is -3.34. This means that the odds will be changed by a value equal to Euler's number e raised to the power equal to the estimate i.e. e^-3.34 = 0.035.   
This essentially mean that one % increase in the yield increases the odds or the probability of tightening by 0.035. 
Accordingly, the 6 month, 2year and 30year yield's have the most impact on the probability of tightening in the market. These could possibly be proxies for short, mid and long term bonds that the Fed uses to decide on fiscal policy.  

The model's accuracy in prediction is contingent on the volatility in the rates. As the rates move more frequently in the intial years (80s), the fitted values of odds are also changing rapidly. In the more recent years, it appears that the markets have been comparitively more stable with less volatility on average leading to relatively less volatile fitted values.  

#Step 5

###5.1 and 5.2 Estimating the Full Linear Regression Model, extracting Coeff, R2 and Degrees of Freedo

```{r eval=TRUE}
#DataPreparation
AssignmentDataRegressionComparison<-data.matrix(AssignmentData[,-c(9,10)],rownames.force="automatic")
AssignmentDataRegressionComparison<-AssignmentData[,-c(9,10)]

#Model Fitting
RegressionModelComparison.Full<-lm(Output1~.,data=AssignmentDataRegressionComparison)

#Extracting Required Charcateristics
print('Coefficients:')
summary(RegressionModelComparison.Full)$coefficients

print('Extent of Variance Explained:') 
cbind('R2'=summary(RegressionModelComparison.Full)$"r.squared", 'Adj.R2'=summary(RegressionModelComparison.Full)$"adj.r.squared")

print('Degrees of Freedom:')  
summary(RegressionModelComparison.Full)$df
```  

###5.3 Interpretation of Full Model   
1. A clear case of Over fitting is visible here as both the R2 and Adj R2 values are equal to 1.We need to drop predictors to arrive at the ideal number that yields a sufficiently high R2 value.  
2. This is an "essentially perfect fit" with all P values being highly significant suggesting that all the predictors are contributing towards additional R2 improvement marginally.
3. This prompts us to look at the correlations between the variables as well as whether any single predictor is an exact predictor of the output. In our case we are aware that the 3YR yield alone can explain as much as 99.79% of the variance in output data

###5.4 and 5.5 Fitting the Null Model and extracting R2, DF and Coefficients

```{r eval=TRUE}
#Model Fitting
RegressionModelComparison.Null<- lm (Output1~1,data=AssignmentDataRegressionComparison)

#Extracting Required Characteristics
print('Coefficients:')
summary(RegressionModelComparison.Null)$coefficients

print('Extent of Variance Explained:') 
cbind('R2'=summary(RegressionModelComparison.Null)$"r.squared", 'Adj.R2'=summary(RegressionModelComparison.Null)$"adj.r.squared")

print('Degrees of Freedom:')  
summary(RegressionModelComparison.Null)$df

```  

###5.6 Interpretation of 0/Absent R2 Value  

In an intercept only model, the value of the fitted value is a constant equal to the mean value of the dependent variable.   
Sum of Squares of the Model is the sum of squared difference between predicted values and mean of dependent variable. Here both are equal, making SSM=0.    
As R2 is a ratio of this explained variance and the total variance, and R2 is always 0.  
Thus, in an intercept only model, reporting R2 does not add value.

###5.7 Anova of Full and Null Models
```{r eval=TRUE}
anova(RegressionModelComparison.Null,RegressionModelComparison.Full)
```  

The Anova comparison yields the information that the Full model is significantly better than the null model in predicting the output. The p value associated with the models' F test is ~0 keeping in line with the t values of the individual predictors in the linear regression calculated above. As the full model is over fit with R2=1, the Residual sum of squares is reduced to 0 effectively.  

###5.8 and 5.9 Selecting the Best Fit Model

a. Attempting the Drop 1 command

```{r eval=TRUE}  
drop1(RegressionModelComparison.Full)
```  

The error message suggests that the model is a perfect fit and all predictors are essential.  
Infering from the AIC values also we can state that there are no predictrors dropping which will reduce the AIC values and thus the drop1 function in this case is essentially unusable.In general, if one or more of the predictor variables is an almost perfect predictor of the response, we don't have much room to select different models.  

b.Attempting the Add1 command instead.    

We begin with the null model and add variables one after another:

```{r eval=TRUE}
myscope<-names(AssignmentDataRegressionComparison)[-which(names(AssignmentDataRegressionComparison)=="Output1")]
myscope
RMC2<-lm(Output1~1, data = AssignmentDataRegressionComparison)
summary(RMC2)$r.squared
add1(RMC2, scope = myscope)
```
We know that R2 for an intercept only model is zero.  
Looking at AIC values, Since 3YR yields' as predictor seems to have most impact on AIC, we add 3YR as a predictor to our model.  

```{r eval=TRUE}
RMC2.3Y<-lm(Output1~USGG3YR, data=AssignmentDataRegressionComparison)
myscope=myscope[-which(myscope=="USGG3YR")]
summary(RMC2.3Y)$r.squared
add1(RMC2.3Y,scope=myscope)
```

  As seen in the inital steps, 3YR as predictor explains 99.79% of the variance. However, the AIC bvalues suggest that taking 3M data into account can improve the output and thus we try with the next model.  

```{r eval=TRUE}
RMC2.3Y3M<-lm(Output1~USGG3YR+USGG3M, data=AssignmentDataRegressionComparison)
summary(RMC2.3Y3M)$r.squared
myscope=myscope[-which(myscope=="USGG3M")]
add1(RMC2.3Y3M,scope=myscope)
```  

We see that the R2 value has improved to 0.9986. This is fairly high. However, if we wish to go closer to R2 value of 1, we can further add 10YR data to improve the prediction.

```{r eval=TRUE}
RMC2.3Y3M10Y<-lm(Output1~USGG3YR+USGG3M+USGG10YR, data=AssignmentDataRegressionComparison)
summary(RMC2.3Y3M10Y)$r.squared
myscope=myscope[-which(myscope=="USGG10YR")]
add1(RMC2.3Y3M10Y,scope=myscope)
```
  
  We can see that adding 6M data will decrease the AIC further and may take R2 to an over-fit / perfect value of 1. However, we have already achieved a near perfect R2 value of 0.9999 and any further addition of predictors will make the cost of the addition prohibitive for such a small amount of improvement in R ~ 0.00008.  
  At this stage using Step function will not yield any better results as we have already looked at both the directions in the add and drop functions.  
  Thus, we accept this model with predictors 3YR,3M and 10YR as the most efficient model available.  
  
#Step 6

###6.1Rolling Means and Daily Differences

```{r eval=TRUE,results='hide'}
Window.width<-20; Window.shift<-5
library(zoo)

all.means<-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=TRUE, mean)
Count<-1:length(AssignmentDataRegressionComparison[,1])
Rolling.window.matrix<-rollapply(Count,width=Window.width,by=Window.shift,by.column=FALSE,
                                 FUN=function(z) z)
Rolling.window.matrix[1:10,]
```

```{r eval=TRUE, results='hide'}
# Take middle of each window
Points.of.calculation<-Rolling.window.matrix[,10]
# Insert means into the total length vector to plot the rolling mean with the original data
Means.forPlot<-rep(NA,length(AssignmentDataRegressionComparison[,1]))
Means.forPlot[Points.of.calculation]<-all.means[,1]
Means.forPlot[1:50]
cbind(AssignmentDataRegressionComparison[,1],Means.forPlot)[1:10,]
```
```{r  }
plot(Means.forPlot,col="red")
lines(AssignmentDataRegressionComparison[,1])
```  

For Daily difference:  
```{r}  
all.sd<-apply(AssignmentDataRegressionComparison,2,diff)
head(all.sd,10)
```  

###6.2 Rolling Standard Deviation of Daily Difference:
```{r}
rolling.sd <- rollapply(all.sd,width=Window.width,by=Window.shift,by.column=TRUE, sd)
head(rolling.sd)
```


###6.3 Volatility and Rates
```{r}
rolling.dates<-rollapply(AssignmentDataRegressionComparison[-1,],width=Window.width,by=Window.shift,
                         by.column=FALSE,FUN=function(z) rownames(z))
rownames(rolling.sd)<-rolling.dates[,10]
matplot(rolling.sd[,c(1,5,7,8)],xaxt="n",type="l",col=c("black","red","blue","green"))
axis(side=1,at=1:1656,rownames(rolling.sd))
```  
  
  In the intial periods, the rates were considerably higher than the more recent periods. From above plot it is clear that during the same coincidental period, the volatility also spiked.  
  In addition, if we look at local spikes we can see that these events coincide with various major financial market crashes like Black Monday in 1987, the Fed's high rates leading to shocks in the early 80s and the sub-prime crisis more recently.  
  Thus, considering the previous data and graphs, we can state that there is a positive correlation between the rates and volatility as defined by the standard deviation of the daily rate differences.  
  To test this hypothesis, let's define the value of rolling standard deviation, beyond which one can state that the volatility is high.  
  From the graph above, we can see that most values are 0.2 or under. Let us say that a rolling daily difference standard deviation upwards of 0.3 signifies a high volatility period.

```{r}
x<-rolling.sd[,8]
high.volatility.periods<-rownames(rolling.sd)[x >0.3]
low.volatility.periods<-rownames(rolling.sd)[x <= 0.3]

CompHighLow <- rbind(apply(AssignmentDataRegressionComparison[high.volatility.periods,1:7], 2,mean),
                            apply(AssignmentDataRegressionComparison[low.volatility.periods,1:7], 2, mean))
rownames(CompHighLow) <- c('During High Volatility', 'During Low Volatility')
CompHighLow
```   
  
  This further clarifies that the rates during high volatility were considerably higher than during low volatility periods.The rates are 200-300% higherduring periods of high volatility as compared to periods of low volatility. This makes intuitive sense as the Feds would have likely enacted tightening measures during this time to keep the market under control.  
  
 
 
###6.4 Pairs Plot  
```{r results='hide'} 
# Rolling lm coefficients
Coefficients<-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
                        FUN=function(z) coef(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z))))
rolling.dates<-rollapply(AssignmentDataRegressionComparison[,1:8],width=Window.width,by=Window.shift,by.column=FALSE,
                         FUN=function(z) rownames(z))

rownames(Coefficients)<-rolling.dates[,10]
Coefficients[1:10,]
```


###6.4 Pairs Plot of Coefficients
```{r}
# Pairs plot of Coefficients
pairs(Coefficients)
```  
  
  
  1. The diamonds in pairs plot denote the phenomena of coupling and decoupling- during certain time periods the two predictors are either getting coupled and moving together or getting de coupled and going in different directions.  
  2. Such anomalies are evident in the relationship between 3M and 5YR Yields as well as 3M and 30YR yields. Depending on the instance at which the coupling/decoupling happens, these can be interpreted as either clockwise or anti-clockwise loops.  
  3.The relationship between 5YR and 30YR yields is linear and ngative i.e. as the 30YR yield increases the 5 YR yield decreases and vice versa.
  4.The relationship of the intercept with 30YR yield also appears to be roughly a negative linear relationship while that with 3M and 5YR yields is more complex and shows signes of coupling/decoupling loops to an extent
  
###6.5 Pairs plot and Coefficients Plot
```{r}  
  # Plot of coefficients
matplot(Coefficients[,-1],xaxt="n",type="l",col=c("black","red","green"))
axis(side=1,at=1:1657,rownames(Coefficients))

high.slopespread.periods<-rownames(Coefficients)[Coefficients[,3]-Coefficients[,4]>3]
jump.slopes<-rownames(Coefficients)[Coefficients[,3]>3]

```  
  
  The coeffiecients are chronologically- black,red and green for 3M, 5YR and 30YR rates. As observed in pairs plot, in the coeff plot as well where there is a negative correlation betwenn red and green, the two are moving in different directions. In periods where the red line is increasing the black subsides and vice versa.  
  Also,between black and green (5yr,30yr) there are periods of coupling and decoupling in the graph i.e. in periods they are moving up/down together and in others they move independedntly. This a further confirmation of the pairs plot.  
  Thus, both the major observations are consistent with the pairs plot


### 6.6 R-squared

```{r reults='hide'}
r.squared<-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
                     FUN=function(z) summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$r.squared)
r.squared<-cbind(rolling.dates[,10],r.squared)
r.squared[1:10,]
```

```{r}
plot(r.squared[,2],xaxt="n",ylim=c(0,1))
axis(side=1,at=1:1657,rownames(Coefficients))
```
```{r}
(low.r.squared.periods<-r.squared[r.squared[,2]<.9,1])
```  
  
  The R2 value suddenly decreasing signals a break from linearity at that point.  
  This is a consequence of sudden market shocks or changes in sentiment that were unforeseen by the fed or the market apriori.  Thus from the dates returned and the points on the graph we can hypothesize that the model breaks from linearity in the immediate lead up to major market events and can thus be utilized by market watchers to take protective measures. We can see this in the fact that the R2 droped in June 1987, right before Black Monday of Oct.1987. The same holds true to a different degree with 2005 and 2012 data.

### 6.7 P-values

```{r}
Pvalues<-rollapply(AssignmentDataRegressionComparison,width=Window.width,by=Window.shift,by.column=FALSE,
                   FUN=function(z) summary(lm(Output1~USGG3M+USGG5YR+USGG30YR,data=as.data.frame(z)))$coefficients[,4])
rownames(Pvalues)<-rolling.dates[,10]
matplot(Pvalues,xaxt="n",col=c("black","blue","red","green"),type="o")
axis(side=1,at=1:1657,rownames(Coefficients))
```  

```{r results='hide'}
rownames(Pvalues)[Pvalues[,2]>.5]

rownames(Pvalues)[Pvalues[,3]>.5]

rownames(Pvalues)[Pvalues[,4]>.5]
```

  On average across the entire timeline, all the p-values were seen to be highly significant, however on a rolling basis it is clear that from time to time certain predictors lose their predictive power and can be dropped from the model.  
  When the p value jumps up, it shows that during that time the particular predictor becomes irrelevant. For example in this plot, predictor 3 (5YR yield) jumps up only during black Monday and again in the dotcom bubble of late 1990s.This would suggest that this yield would be a good candidate for a best-fit model, however as we have seen the add function suggests otherwise.  
  The predictor 2 (3M yield) was largely a significant predictor of output right until around the end of the last decade.It can be hypothesized that this latest few years have had higher volatility in this predictor leading it to break from the line traced by the output. This is reaffirmed in the plot of 3M vs Output1 above.  
  The predictor4 (30yr rates) seems to be the one with the least amount of time as a significant predictor and can thus be dropped from the model. This is true using AIC as well.  


#Step 7

###7.1 Manual covariance Matrix  
  
  The covariance Matrix is (Transpose of Centered Y)dot(Centered Y)/(number of samples-1)
```{r}
AssignmentData<-data.matrix(AssignmentData[,1:7],rownames.force="automatic")
colCount<-ncol(AssignmentData)
rowCount<-nrow(AssignmentData)
Yo <- AssignmentData
Ybar <- colMeans(AssignmentData)
for (i in 1:colCount){
  Yo[,i] <- AssignmentData[,i] -Ybar[i]
}
#head(Yo)
Manual.Covariance.Matrix <- 1/(rowCount-1)* t(Yo) %*% Yo
Manual.Covariance.Matrix
```

###7.2 and 7.3 Using the Cov() function and Analyzing Data

```{r}
Covariance.Matrix<-cov(AssignmentData)
Covariance.Matrix
```

```{r}
Maturities<-c(.25,.5,2,3,5,10,30)
contour(Maturities,Maturities,Covariance.Matrix)
```  

The covariance between shorter maturity rates is visibly higher than with the longer maturity rates(10YR,30YR).  
For all the predcitors the covariance values increases from 3M to 2YR rates and then decreases from 3YR to 30YR rates.That is why the global peak is at the 2YR level. 
The 2YR rate has the highest covariance across all the predictors.  
In addition, the contour lines are more concentrated in the 0-5YR range because of the higher covariance as well more such predictors while the plot gets less dense on the top right. 
Considering the correlation between terms and the high covariance, it is likely that interaction effects are playing a part in making this a perfect fit model.  

###7.4 PCA Manually

```{r}
EigenVal<-eigen(Covariance.Matrix)
EigenVal
```  
  
 Since Yo= F. L Transpose 

```{r}
L<-data.matrix(cbind(EigenVal$vectors))
Facto<-(Yo) %*% (L)
head(Facto)
```  
  
###7.5 Factor Plot

```{r}
barplot(EigenVal$values/sum(EigenVal$values),width=2,col = "black",
        names.arg=c("F1","F2","F3","F4","F5","F6","F7"))
```  
  
###7.6 Loading Plot  
```{r}
matplot(Maturities,L[,1:3],type="l",lty=1,col=c("black","red","green"),lwd=3)
```  
  
###7.7 Interpretation of Factors  
  
Loadings are essentially weights for a linear combination corresponding to the projection of the original points onto the new axis selected. As loadings are constant over time, each Factor makes an adjustment corresponding with its shape:  

1. Factor 1 (black line) loading is constantly negative with small movements. This means that for each individual predictor/maturity, this factor will merely move it in a parallel way downwards i.e. this is a "Parallel Shift" mode of movement of the graph.  
2. Factor 2(red line) loading is negative for shorter maturities (reaching zero~3YR) and positive for longer maturities. This would mean that it would move the linear combination line in a twisted/tilted way with one part of it going down and the other up based on the nature of the values.Thus, this factor causes a "Tilt" mode of movement.  
3. Factor 3 (green line) loading crosses the zero line twicw at first going from positive to negative in the period of 0-2 years and then upwards thereafter. Thus when these loadings are multiplied with the factors, the linear combination will be an initial upward surge in the shortest maturities, followed by a dip in the other short. maturities and eventually another increase in the longer maturities. This would make the mode of movement "Bow" shaped with two local maximas and one minima.  

### 7.8 Factor Plot  
  
```{r}
matplot(Facto[,1:3],type="l",col=c("black","red","green"),lty=1,lwd=3)
```


```{r}
L[,1] <- -L[,1]
Facto[,1] <- -Facto[,1]
matplot(Facto[,1:3],type="l",col=c("black","red","green"),lty=1,lwd=3)
matplot(Maturities,L[,1:3],type="l",lty=1,col=c("black","red","green"),lwd=3)
```

### 7.9 Factors 1,2  Plotted against each other 
```{r}
plot(Facto[,1],Facto[,2],type="l",lwd=2)
```


### 7.10 Anlysis of factors plot
   
1. Across the entire time series, the factors have a correlation of 0, however if we look closely at smaller chunks of data, we can see specific correlations developing between the factors. For example, in the latest period the factors are showing negative correlation (left most area). However this is taking the negative of factor1. Thus the actual factors are positively correlated.  
2. The extent of movement of the graph during small periods of time can be considered as a proxy for the volatility in that period. This is consistent with our previous observations and we can see that in the earlier periods (1980s-90s) the volatility was far higher than in the  21st century.  
3.Similar to the pairs plot of the predictors earlier, the factor plots also display signs of coupling and decoupling. The two factors move in clockwise and anticlockwise loops from time to time impacted by market externalities.For example, between the 1000th and 2000th days there is an anticlockwise loop. This suggests that for thse periods of time that the factors are moving together they are coupled/engaged and thereafter they get decoupled and behave independently.  

###7.11 Adjustments and their interpretation  

```{r}
AssignmentData<-data.matrix(AssignmentData[,1:7],rownames.force="automatic")
Factors<-Facto
Loadings<-L
OldCurve<-AssignmentData[135,]
NewCurve<-AssignmentData[136,]
CurveChange<-NewCurve-OldCurve
FactorsChange<-Factors[136,]-Factors[135,]
ModelCurveAdjustment.1Factor<-OldCurve+t(Loadings[,1])*FactorsChange[1]
ModelCurveAdjustment.2Factors<-OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]
ModelCurveAdjustment.3Factors<-OldCurve+t(Loadings[,1])*FactorsChange[1]+t(Loadings[,2])*FactorsChange[2]+
  t(Loadings[,3])*FactorsChange[3]
matplot(Maturities,
        t(rbind(OldCurve,NewCurve,ModelCurveAdjustment.1Factor,ModelCurveAdjustment.2Factors,
                ModelCurveAdjustment.3Factors)),
        type="l",lty=c(1,1,2,2,2),col=c("black","red","green","blue","magenta"),lwd=3,ylab="Curve Adjustment")
legend(x="topright",c("Old Curve","New Curve","1-Factor Adj.","2-Factor Adj.",
                      "3-Factor Adj."),lty=c(1,1,2,2,2),lwd=3,col=c("black","red","green","blue","magenta"))
```
  
The Factors act as shape changers for the Curve with mode of change as :  
1.factor 1: Shift; facor 2: Tilt; factor 3: Bow -- These were derived from the study of the loading plots  
2. The new curve is the linear combination of these three modes of movement times the factors- thus we can break down the movement from old curve to new curve in three distict steps viz. Shift, Tilt and Bow  
3.When we look at the plot of factor 1 alone, i.e. the green line, we can see an almost parallel movement from the black line upwards. We can correlate this to the loadings that were almost a flat line below zero which we then reversed to above zero.  
4.The Blue dotted line is a combination of shift plus tilt on the old curve. If we want to isolate the action of factor 2 alone we look at the change between green and blue lines. As expected, it is just a tilt along the vertical axis pssing roughly through the 2YR point where the loadings ~0. All points before this have been raised as compared to the green line and points after this have been pushed downwards.  
5.Finally, the Pink line which is a close approximation of the final red line of the new curve brings into effect all three principal modes of movement. Comparing the pink and blue lines we can see the bow effect, i.e. initially it raises, then lowers and then lowers the the line again as compared to the blue line.  
6.The unexplained minor differences between red and pink lines is owing to the other components not considered here.  

```{r}
rbind(CurveChange,ModelCurveAdjustment.3Factors-OldCurve)
```
  
  
### 7.12 Comparing against Pricomp

```{r}
PCA.Yields<-princomp(AssignmentData)
names(PCA.Yields)
# Check that the loadings are the same
cbind(PCA.Yields$loadings[,1:3],Maturities,EigenVal$vectors[,1:3])
AllTrues<-FALSE
for (i in (1:nrow(as.matrix(PCA.Yields)))){
  for(j in (1:3)){
    if (round(abs(PCA.Yields$loadings[i,j]),7)==round(abs(EigenVal$vectors[i,j]),7)) {
      AllTrues<-TRUE
    }
  }
}
PCA.Yields$loadings[,1:3]
AllTrues
```
  
  Applying the loop to check through absolute values of all the loadings, it is confirmed that the values of loadings are indeed the same as calculated manually and through the Princomp function.


```{r}
matplot(Maturities,PCA.Yields$loadings[,1:3],type="l",col=c("black","red","green"),lty=1,lwd=3)
```

```{r}
# Change the signs of the 1st factor and the first loading
PCA.Yields$loadings[,1]<--PCA.Yields$loadings[,1]
PCA.Yields$scores[,1]<--PCA.Yields$scores[,1]
matplot(Maturities,PCA.Yields$loadings[,1:3],type="l",col=c("black","red","green"),lty=1,lwd=3)
#factor plot
matplot(PCA.Yields$scores[,1:3],type="l",col=c("black","red","green"),lwd=3,lty=1)
```
  
  These graphs also confirm that the loadings obtained are the same in both cases.

###7.13 Comparing Factor 1 from manual and Princomp against Output1
```{r}
AssignmentData.Output<-AssignmentDataRegressionComparison$Output1
matplot(cbind(PCA.Yields$scores[,1],AssignmentData.Output,Factors[,1]),type="l",col=c("black","red","green"),lwd=c(3,2,1),lty=c(1,2,3),ylab="Factor 1")
```

  
  Plotting the Factor 1 from our manual PCA, Pincomp against Output 1 from the data reveals that the unknown Output is actually factor 1.  
  Since this is infact a linear combination of the predictors, the initial response that the mode was a perfect fit, makes sense now.  
  

Compare the regression coefficients from Step 2 and Step 3 with factor loadings.


  
```{r}
#First, look at the slopes for AssignmentData.Input~AssignmentData.Output
t(apply(AssignmentData, 2, function(AssignmentData.col) lm(AssignmentData.col~AssignmentData.Output)$coef))
```

```{r}
#This shows that the zero loading equals the vector of intercepts of models Y~Output1, where Y is one of the columns of yields in the data.
#Also, the slopes of the same models are equal to the first loading.
cbind(PCA.Yields$center,PCA.Yields$loadings[,1])
```

```{r}
#Check if the same is true in the opposite direction: is there a correspondence between the coefficients of models Output1~Yield and the first loading.

AssignmentData.Centered<-t(apply(AssignmentData,1,function(AssignmentData.row) AssignmentData.row-PCA.Yields$center))
#dim(AssignmentData.Centered)
t(apply(AssignmentData.Centered, 2, function(AssignmentData.col) lm(AssignmentData.Output~AssignmentData.col)$coef))
```

```{r}
#To recover the loading of the first factor by doing regression, use all inputs together.

Comp<-rbind(t(lm(AssignmentData.Output~AssignmentData.Centered)$coef)[-1],PCA.Yields$loadings[,1])
rownames(Comp) <- c('Regression against Centered', 'PCA Loadings ')
Comp
```  
  
  This clarifies that the PCA loadings are  the coefficients from the linear regression carried out on a centered predictors matrix. This further gives us clarity about the fact that PCA is indeed a  linear regression that plots the existing points onto a new basis.  
  
  Thus, the output can be visualized as  being a portfolio of investments diversified over the various maturities. The output would thus be the daily rate earned on the entire portfolio!


#                                                        ------END-------

